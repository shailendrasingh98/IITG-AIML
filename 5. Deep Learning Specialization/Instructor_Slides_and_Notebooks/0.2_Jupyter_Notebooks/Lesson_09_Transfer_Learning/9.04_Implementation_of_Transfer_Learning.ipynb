{"cells":[{"cell_type":"markdown","source":["## __Transfer Learning__\n","- Transfer learning refers to a technique in machine learning where a pre-trained model, typically trained on a large dataset, is used as a starting point for solving a different but related task.\n","- It involves using models that were trained on one problem as a starting point for solving a related problem.\n","- It is flexible, allowing the use of pre-trained models directly, as feature extraction preprocessing, and integrated into entirely new models.\n","\n"],"metadata":{"id":"15F8jGyUdtrG"}},{"cell_type":"markdown","source":["In this demo, we will learn how to utilize transfer learning with the VGG16 model to adapt pre-trained features for a new classification task, highlighting efficient model adaptation without extensive new training."],"metadata":{"id":"qVynXmOEGE0Y"}},{"cell_type":"markdown","source":["## Steps to be followed:\n","1. Import the required libraries\n","2. Add classifier layers\n","3. Perform preprocessing and feature extraction"],"metadata":{"id":"LgCmoesorvHh"}},{"cell_type":"markdown","source":["### Step 1: Import the required libraries\n","\n","- The **from tensorflow.keras.utils import load_img** loads an image file from the file system.\n","\n","- The **from tensorflow.keras.utils import img_to_array** converts an image loaded with load_img into a NumPy array.\n","\n","- The **from keras.applications.vgg16 import preprocess_input** preprocesses the input image array before feeding it to the VGG16 model. VGG16 expects the input images to be preprocessed in a specific way.\n","\n","- The **from keras.applications.vgg16 import VGG16** imports the VGG16 model architecture. VGG16 is a popular convolutional neural network model pre-trained on the ImageNet dataset for image classification."],"metadata":{"id":"cKMViz1Ngp8Y"}},{"cell_type":"code","execution_count":null,"source":["from tensorflow.keras.utils import load_img\n","from tensorflow.keras.utils import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","\n","from keras.applications.vgg16 import VGG16\n"],"outputs":[],"metadata":{"id":"5woHmJrJdtrK"}},{"cell_type":"markdown","source":["### Step 2: Add classifier layers\n","- It demonstrates how to load a pre-trained VGG16 model without its classifier layers and then add new custom classifier layers on top of it.\n","- The new model is defined by connecting the output of the pre-trained VGG16 model to a flattening layer. It is followed by a dense layer with 1024 units and ReLU activation and a dense layer with 10 units and Softmax activation for multi-class classification.\n","- The model summary provides an overview of the architecture and layer configurations."],"metadata":{"id":"K_860Ql8kwv-"}},{"cell_type":"code","execution_count":null,"source":["\n","from keras.models import Model\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","\n","base_model = VGG16(include_top=False, input_shape=(300, 300, 3))\n","flat1 = Flatten()(base_model.layers[-1].output)\n","class1 = Dense(1024, activation='relu')(flat1)\n","output = Dense(10, activation='softmax')(class1)\n","\n","model = Model(inputs=base_model.inputs, outputs=output)\n","model.summary()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_5 (InputLayer)        [(None, 300, 300, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 300, 300, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 300, 300, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 150, 150, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 150, 150, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 150, 150, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 75, 75, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 75, 75, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 75, 75, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 75, 75, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 37, 37, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 37, 37, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 37, 37, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 37, 37, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 18, 18, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 41472)             0         \n","                                                                 \n"," dense_4 (Dense)             (None, 1024)              42468352  \n","                                                                 \n"," dense_5 (Dense)             (None, 10)                10250     \n","                                                                 \n","=================================================================\n","Total params: 57193290 (218.18 MB)\n","Trainable params: 57193290 (218.18 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h95JEG-VdtrM","outputId":"fba7ca8a-63fe-4ef8-89f4-8eee0fcb741b","executionInfo":{"status":"ok","timestamp":1719144858159,"user_tz":-330,"elapsed":560,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}}}},{"cell_type":"markdown","source":["**Observation**\n","- Running the example means that the new model is ready for training and summarizes the model architecture.\n","- The output of the last pooling layer is flattened, and the new fully connected layers are added.\n","- The weights of the VGG16 model and the new model will all be trained together on the new dataset."],"metadata":{"id":"E3tLfn7WdtrN"}},{"cell_type":"markdown","source":["### Step 3: Perform preprocessing and feature extraction\n","- The image is loaded from a file and preprocessed to meet the input requirements of the VGG16 model (resizing, converting to a numpy array, and reshaping).\n","\n","- The modified model predicts and extracts features from the input image, resulting in a feature vector with a specific shape."],"metadata":{"id":"3DWAS8owlYbP"}},{"cell_type":"code","execution_count":null,"source":["image = load_img('dog.jpg', target_size=(300, 300))\n","image = img_to_array(image)\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","image = preprocess_input(image)\n","\n","# Predict using the correct model\n","features = model.predict(image)\n","print(\"Output shape:\", features.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 1s/step\n","Output shape: (1, 10)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pn9hQjZCdtrN","outputId":"34e66e7c-4c8f-46ac-d9b3-e70f23e96643","executionInfo":{"status":"ok","timestamp":1719144896843,"user_tz":-330,"elapsed":1599,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"}}}},{"cell_type":"markdown","source":["**Observation**\n","\n","- The VGG16 model weights are downloaded and loaded successfully, and the extracted features from the input image have a shape of (1, 10).\n","- The final layer in the model is a Dense layer with 10 units and a softmax activation function. This is designed to output the probabilities of each of the 10 classes for the input image. Since you have one image as input, the batch size is 1, leading to the output shape of (1, 10)."],"metadata":{"id":"Jwvk_m95mR7D"}},{"cell_type":"code","source":[],"metadata":{"id":"gbhz9AgYB2LX"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}