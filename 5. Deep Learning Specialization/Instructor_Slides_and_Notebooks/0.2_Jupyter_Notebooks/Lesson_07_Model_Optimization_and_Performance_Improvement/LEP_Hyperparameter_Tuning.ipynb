{"cells":[{"cell_type":"markdown","metadata":{"id":"SDt5EKeIdYdr"},"source":["# __Hyperparameter Tuning__\n","- Hyperparameter tuning is the process of systematically searching for the best combination of hyperparameter values for a machine learning model.\n","- It involves selecting a subset of hyperparameters and exploring different values for each hyperparameter to find the configuration that optimizes the model's performance on a given dataset.\n","\n","Let's understand how it works."]},{"cell_type":"markdown","metadata":{"id":"ovC08-LtG9-v"},"source":["## Steps to be followed:\n","1. Import the required libraries\n","2. Load the dataset and standardize it\n","3. Build and Train a Basic Deep Learning Model\n","4. Define the HyperModel\n","5. Instantiate the Tuner and Perform Hyperparameter Tuning\n","6. Evaluate Both Models Using Multiple Metrics"]},{"cell_type":"markdown","metadata":{"id":"SHDFVGGzdYdv"},"source":["### Step 1: Import the required libraries\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22904,"status":"ok","timestamp":1719220342797,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"},"user_tz":-330},"id":"hfbZk64D8wSB","outputId":"14121b15-dced-44b2-b8fa-e15196149e57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n","Collecting kt-legacy (from keras-tuner)\n","  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.6.2)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"]}],"source":["!pip install keras-tuner"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1719221527277,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"},"user_tz":-330},"id":"qHsqGlzNdYdw"},"outputs":[],"source":["import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras import optimizers\n","\n","import keras_tuner\n","from keras_tuner import HyperModel\n","from keras_tuner.tuners import RandomSearch\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"]},{"cell_type":"markdown","metadata":{"id":"8lTAZH6jGVcO"},"source":["### Step 2: Load the dataset and standardize it\n","- In this code, the breast cancer dataset is loaded using the **load_breast_cancer** function.\n","- The features are stored in the **X** variable, and the corresponding labels are stored in **y**.\n","- Split the dataset into training and testing sets, then standardizes the training data to have zero mean and unit variance, applying the same transformation to the test data."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":437,"status":"ok","timestamp":1719220365154,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"},"user_tz":-330},"id":"6JwDj1KddYdz"},"outputs":[],"source":["# Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","train_sc = scaler.fit_transform(X_train)\n","test_sc = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"yRyQurw2jsMi"},"source":["### Step 3: Build and Train a Basic Deep Learning Model\n","\n","- Constructs a sequential neural network with two hidden layers of 32 and 16 neurons respectively, using ReLU activation, and a dropout layer to reduce overfitting. The output layer uses a sigmoid activation function for binary classification.\n","\n","- Prepares the model for training by specifying the Adam optimizer, binary cross-entropy loss function for binary classification, and tracks the accuracy metric.\n","\n","- Fits the model on the standardized training data for 100 epochs, using 10% of it as a validation set to monitor performance, without verbosity to minimize output during training.\n","\n","- Assesses the model's performance on the standardized test data, obtaining the loss and accuracy, then prints the accuracy to give an indication of how well the model predicts unseen data."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12339,"status":"ok","timestamp":1719220387177,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"},"user_tz":-330},"id":"-k-7dw0udYd0","outputId":"31cc39a5-eb8a-491b-cd8f-7d34ee40b18e"},"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 0s 3ms/step - loss: 0.0960 - accuracy: 0.9737\n","Basic Model Accuracy:  0.9736841917037964\n"]}],"source":["# Build a basic model\n","basic_model = Sequential([\n","    Dense(32, activation='relu', input_shape=(train_sc.shape[1],)),\n","    Dropout(0.2),\n","    Dense(16, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","basic_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","basic_model.fit(train_sc, y_train, epochs=100, validation_split=0.1, verbose=0)\n","\n","# Evaluate the model on the test set\n","basic_loss, basic_accuracy = basic_model.evaluate(test_sc, y_test)\n","print(\"Basic Model Accuracy: \", basic_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"qZsObONRki1-"},"source":["### Step 4:  Define the HyperModel\n","Defines a custom class MyHyperModel that extends the HyperModel class from Keras Tuner, used for hyperparameter tuning.\n","\n","- The class `MyHyperModel` is designed to construct a neural network model dynamically, with varying hyperparameters.\n","- The `__init__` method initializes the class with an `input_shape`, which is the shape (number of features) of the input data that the model will expect. This is stored as a class attribute to be used later in building the model.\n","\n","- The build method creates a neural network model architecture with tunable hyperparameters.\n","- `hp.Int('units', min_value=10, max_value=100, step=10)` This line specifies that the number of units in the Dense layers should be treated as a hyperparameter, with possible values ranging from 10 to 100 in steps of 10.\n","\n","- `hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)` This specifies that the dropout rate should also be a hyperparameter, ranging from 0.0 to 0.5 with a step of 0.1.\n","\n","- `model.add(Dense(1, activation='sigmoid'))` Adds an output layer with a single unit and sigmoid activation suitable for binary classification.\n","\n","- Learning rate for the Adam optimizer is configured with another tunable parameter `(hp.Float('learning_rate', ...))` which varies logarithmically from 0.0001 to 0.01.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":460,"status":"ok","timestamp":1719221150498,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"},"user_tz":-330},"id":"7EMzKZEJdYd1"},"outputs":[],"source":["class MyHyperModel(HyperModel):\n","    def __init__(self, input_shape):\n","        self.input_shape = input_shape\n","\n","    def build(self, hp):\n","        model = Sequential()\n","        model.add(Dense(\n","            units=hp.Int('units', min_value=10, max_value=100, step=10),\n","            activation='relu', input_shape=(self.input_shape,)\n","        ))\n","        model.add(Dropout(\n","            hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)\n","        ))\n","        model.add(Dense(\n","            units=hp.Int('units', min_value=10, max_value=100, step=10),\n","            activation='relu'\n","        ))\n","        model.add(Dense(1, activation='sigmoid'))\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adam(\n","                hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n","            loss='binary_crossentropy',\n","            metrics=['accuracy']\n","        )\n","        return model"]},{"cell_type":"markdown","metadata":{"id":"AHDK8vEbl9jU"},"source":["### Step 5: Instantiate the Tuner and Perform Hyperparameter Tuning\n","- Conducts hyperparameter tuning using Keras Tuner's RandomSearch, optimizing the neural network's configuration to maximize validation accuracy by testing different combinations of model parameters and identifying the best performing model.\n","\n","- It sets up a hyperparameter optimization process targeting the validation accuracy for a model defined by hypermodel.\n","\n","- The process will try up to 10 different sets of hyperparameters, running each configuration twice to ensure stability in the reported performance metrics, all within the specified project directory for organized storage and potential review.\n","\n","- This approach is useful for exploring a potentially vast hyperparameter space more efficiently than exhaustively testing all combinations."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122907,"status":"ok","timestamp":1719221293117,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"},"user_tz":-330},"id":"QfeRXRDtdYd2","outputId":"119699fc-ef07-4640-89f6-c5d91a65f161"},"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 10 Complete [00h 00m 12s]\n","val_accuracy: 0.9780219793319702\n","\n","Best val_accuracy So Far: 0.9890109896659851\n","Total elapsed time: 00h 02m 02s\n"]}],"source":["# Assuming 'train_sc' and 'y_train' are defined as your scaled training data and labels\n","input_shape = train_sc.shape[1]  # Extract the number of features\n","\n","# Create an instance of the HyperModel\n","hypermodel = MyHyperModel(input_shape=input_shape)\n","\n","# Instantiate the tuner\n","tuner = RandomSearch(\n","    hypermodel,\n","    objective='val_accuracy',\n","    max_trials=10,\n","    executions_per_trial=2,\n","    directory='tuner_data',\n","    project_name='breast_cancer_optimization'\n",")\n","\n","# Perform hyperparameter tuning\n","tuner.search(train_sc, y_train, epochs=50, validation_split=0.2)\n","\n","# Get the best model\n","best_model = tuner.get_best_models(num_models=1)[0]"]},{"cell_type":"markdown","metadata":{"id":"L1qQrUoWK0Xr"},"source":["#### **Observation:**\n","\n","- Trial 10 Complete: Indicates the completion of the 10th trial in the hyperparameter tuning process.\n","\n","- val_accuracy: Indicates the validation accuracy achieved by the model configuration tested in the 10th trial. It indicates how well the model performed on the validation set, which is a subset of the training data not used for training the model but for evaluating its performance.\n","\n","- Best val_accuracy So Far: 0.9890109896659851: Up to this point in the tuning process, this is the highest validation accuracy achieved across all trials. This suggests that the model configuration from the 10th trial is currently the best performer among all configurations tested."]},{"cell_type":"markdown","metadata":{"id":"S3w1EHOEnLWd"},"source":["### Step 6: : Evaluate Both Models Using Multiple Metrics"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1719221536717,"user":{"displayName":"Aleena Raj","userId":"16635257578699511263"},"user_tz":-330},"id":"ef-uEPxAdYd2","outputId":"dfc59575-b457-4d6d-e60b-039e9775eeee"},"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 0s 3ms/step\n","4/4 [==============================] - 0s 3ms/step\n","Basic Model Classification Report:\n","              precision    recall  f1-score   support\n","\n","      Benign       0.98      0.95      0.96        43\n","   Malignant       0.97      0.99      0.98        71\n","\n","    accuracy                           0.97       114\n","   macro avg       0.97      0.97      0.97       114\n","weighted avg       0.97      0.97      0.97       114\n","\n","Basic Model ROC AUC: 0.9931215198165739\n","Best Model Classification Report:\n","              precision    recall  f1-score   support\n","\n","      Benign       1.00      0.95      0.98        43\n","   Malignant       0.97      1.00      0.99        71\n","\n","    accuracy                           0.98       114\n","   macro avg       0.99      0.98      0.98       114\n","weighted avg       0.98      0.98      0.98       114\n","\n","Best Model ROC AUC: 0.9934490664919752\n"]}],"source":["# Predict probabilities for the basic and best models\n","basic_predictions_proba = basic_model.predict(test_sc)\n","basic_predictions = (basic_predictions_proba > 0.5).astype(int)\n","\n","best_predictions_proba = best_model.predict(test_sc)\n","best_predictions = (best_predictions_proba > 0.5).astype(int)\n","\n","# Print classification report for basic model\n","print(\"Basic Model Classification Report:\")\n","print(classification_report(y_test, basic_predictions, target_names=['Benign', 'Malignant']))\n","\n","# Calculate and print ROC AUC for the basic model\n","basic_auc = roc_auc_score(y_test, basic_predictions_proba)\n","print(\"Basic Model ROC AUC:\", basic_auc)\n","\n","# Print classification report for best model\n","print(\"Best Model Classification Report:\")\n","print(classification_report(y_test, best_predictions, target_names=['Benign', 'Malignant']))\n","\n","# Calculate and print ROC AUC for the best model\n","best_auc = roc_auc_score(y_test, best_predictions_proba)\n","print(\"Best Model ROC AUC:\", best_auc)\n"]},{"cell_type":"markdown","metadata":{"id":"98VeSPHzqMdK"},"source":["#### **Observation:**\n","\n","**Basic Model Results:**\n","\n","- Classification Report:\n","    - Benign: Precision of 0.98, recall of 0.95, and F1-score of 0.96. High precision and recall suggest the model effectively identifies benign cases.\n","    - Malignant: Precision of 0.97, recall of 1, and F1-score of 0.98. This shows excellent performance in identifying malignant cases with very high precision.\n","    - Overall Accuracy: 97%, indicating a high overall rate of correct predictions.\n","- ROC AUC: 0.99, which is very close to 1. This score indicates an excellent ability to discriminate between the benign and malignant classes.\n","\n","**Best Model Results:**\n","\n","- Classification Report:\n","    - Benign: Perfect precision of 1.00 and recall of 0.95, resulting in an F1-score of 0.98.\n","    - Malignant: Precision of 0.97 and a perfect recall of 1.00, with an F1-score of 0.99. This configuration slightly improves in identifying all malignant cases compared to the basic model.\n","    - Overall Accuracy: 98%, a slight improvement over the basic model.\n","- ROC AUC: 0.99, indicating a marginal but notable improvement in discriminative ability over the basic model.\n","\n","As per the results, the best model, which underwent hyperparameter tuning, shows improved performance metrics across most areas compared to the basic model. While both models perform exceptionally well, the best model's perfect recall for malignant cases and higher overall ROC AUC suggest it is slightly more reliable, particularly in scenarios where failing to detect malignant cases (false negatives) is critically risky"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}